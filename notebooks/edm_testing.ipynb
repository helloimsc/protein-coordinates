{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio.PDB import PDBList, PDBParser\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from data_read import *\n",
    "import dgl\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdb_files(sample_size=100):\n",
    "    \"\"\"\n",
    "    Retrieves all PDB IDs available in the PDB.\n",
    "\n",
    "    Returns:\n",
    "        list: List of all PDB IDs.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    pdbl = PDBList()\n",
    "    pdb_ids = pdbl.get_all_entries()\n",
    "    sampled_pdb_ids = np.random.choice(pdb_ids, sample_size, replace=False)\n",
    "    for pdb_id in sampled_pdb_ids:\n",
    "        pdbl.retrieve_pdb_file(pdb_id, pdir='pdb_files', file_format='pdb')\n",
    "    print(f\"Downloaded {len(sampled_pdb_ids)} PDB files.\")\n",
    "    return pdb_ids\n",
    "\n",
    "download_pdb_files(sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 77 files to ../data/pdb_files/train\n",
      "Moved 20 files to ../data/pdb_files/test\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the paths\n",
    "data_path = '../data/pdb_files/'\n",
    "train_path = os.path.join(data_path, 'train')\n",
    "test_path = os.path.join(data_path, 'test')\n",
    "\n",
    "# Create train and test directories if they don't exist\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "os.makedirs(test_path, exist_ok=True)\n",
    "\n",
    "# Get list of all files in the data_path\n",
    "all_files = [f for f in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, f))]\n",
    "\n",
    "# Split the files into train and test sets\n",
    "train_files, test_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Move the files to the respective directories\n",
    "for file in train_files:\n",
    "    shutil.move(os.path.join(data_path, file), os.path.join(train_path, file))\n",
    "\n",
    "for file in test_files:\n",
    "    shutil.move(os.path.join(data_path, file), os.path.join(test_path, file))\n",
    "\n",
    "print(f\"Moved {len(train_files)} files to {train_path}\")\n",
    "print(f\"Moved {len(test_files)} files to {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.datasets import *\n",
    "from src.dataset.transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_transforms = transforms.Compose([NormalizeCoordinates(), PadDatasetTransform(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:04<00:00, 15.62it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 23.33it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_path = '../data/pdb_files/train'\n",
    "test_data_path = '../data/pdb_files/test'\n",
    "train_dataset = PDBDataset(train_data_path)\n",
    "test_dataset = PDBDataset(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.collate import PreprocessPDB\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=PreprocessPDB().collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=PreprocessPDB().collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.prepare_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model(args, device, dataset_info, dataloaders['train'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
