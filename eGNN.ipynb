{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cff622-fd4b-4efa-a191-7002af06593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eGNN\n",
    "import data_read\n",
    "import protein_residues\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from Bio.SeqUtils import seq1, seq3\n",
    "from Bio.PDB import PDBIO, StructureBuilder\n",
    "import gc\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(\"cpu\")\n",
    "pdb_dir ='/mnt/rna01/nico/dynamics_project/learn/eGNN/data/dompdb/'\n",
    "list_path = \"/mnt/rna01/nico/dynamics_project/learn/eGNN/data/clean_pdb_id.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cfe2f34-b69f-4706-82a4-8457eb6430ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std = 10 #The std of protein coords is determined to be ~10 as indicated by data_processing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07660322-3a6d-4bc5-b5a9-948e78119d46",
   "metadata": {},
   "source": [
    "# New Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d6548d2-e61f-41ee-9c4b-ca21e64b7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_res_object(pdb_seq,coords):\n",
    "    residues = []\n",
    "    for i ,aa in enumerate(pdb_seq, start=0):\n",
    "        if aa == \"G\":\n",
    "            res = {\"name\":seq3(aa),\"atoms\":[(\"N\", coords[i,0,:].tolist()), (\"CA\", coords[i,1,:].tolist()), (\"C\", coords[i,2,:].tolist())]}\n",
    "        else:\n",
    "            res = {\"name\":seq3(aa),\"atoms\":[(\"N\", coords[i,0,:].tolist()), (\"CA\", coords[i,1,:].tolist()), (\"C\", coords[i,2,:].tolist()),(\"CB\", coords[i,3,:].tolist())]}\n",
    "        residues.append(res)\n",
    "    return residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12ffbcb-bcdb-4ced-a658-918cb3a4d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pdb(pdb_id,residues):\n",
    "    builder = StructureBuilder.StructureBuilder()\n",
    "    \n",
    "    # Create a structure object\n",
    "    builder.init_structure(\"Predicted eGNN Backbone \")\n",
    "    builder.init_model(0)\n",
    "    builder.init_chain(\"A\")  # Single chain \"A\"\n",
    "    builder.init_seg(\" \")\n",
    "    \n",
    "    for res_id, residue in enumerate(residues, start=1):\n",
    "        builder.init_residue(residue[\"name\"], \" \", res_id, \" \")\n",
    "    \n",
    "        # Add atoms to the residue\n",
    "        for atom_name, coords in residue[\"atoms\"]:\n",
    "            builder.init_atom(atom_name, coords, 1.0, 1.0, \" \", atom_name, res_id, atom_name[0])\n",
    "\n",
    "    structure = builder.get_structure()\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    io.save(pdb_id+\".pdb\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28453142-d0b5-44d9-8387-93e822851126",
   "metadata": {},
   "source": [
    "# Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4cadbb1-99d6-4048-9346-9f3a72db2cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "\n",
    "    def __init__(self, T, b_initial, b_final, device):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.beta = torch.linspace(b_initial,b_final,T).to(device)\n",
    "        \n",
    "    def _CoMGaussNoise(self, x_t1, t1, t2):\n",
    "        if t2 == 0:\n",
    "            a_mul = torch.prod(1-self.beta[0])\n",
    "        else:\n",
    "            a_mul = torch.prod(1-self.beta[t1:t2+1])\n",
    "        eps = torch.normal(mean=torch.zeros_like(x_t1),std=torch.ones_like(x_t1))\n",
    "        x1_mean = torch.mean(x_t1.flatten(end_dim=-2),dim=-2, keepdim=True)[None,:]\n",
    "        eps = eps - x1_mean \n",
    "        x_t2 = torch.sqrt(a_mul)*x_t1 + torch.sqrt(1-a_mul)*eps\n",
    "        return x_t2, eps\n",
    "\n",
    "    def _GaussNoise(self, x_t1, t1, t2):\n",
    "        if t2 == 0:\n",
    "            a_mul = torch.prod(1-self.beta[0])\n",
    "        else:\n",
    "            a_mul = torch.prod(1-self.beta[t1:t2+1])\n",
    "        eps = torch.normal(mean=torch.zeros_like(x_t1),std=torch.ones_like(x_t1))\n",
    "        x_t2 = torch.sqrt(a_mul)*x_t1 + torch.sqrt(1-a_mul)*eps\n",
    "        #Same output has the same shape as the input\n",
    "        return x_t2, eps\n",
    "        \n",
    "    def forward(self, x, h, t2):\n",
    "        x_perturbed, x_eps = self._GaussNoise(x, 0, t2)\n",
    "        h_perturbed, h_eps = self._GaussNoise(h,0,t2)\n",
    "        return x_perturbed, x_eps, h_perturbed, h_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0770438b-83d0-46e8-8697-23acf2e94bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_generation(pdb_path,edge_device=\"cuda\"):\n",
    "    frames, seq = data_read.get_backbone(pdb_path)\n",
    "    frames = torch.from_numpy(frames[0]); seq = seq[0]\n",
    "    frames = frames.to(torch.float32)\n",
    "    n = len(seq)\n",
    "    seq_id = data_read.encode(seq[0])\n",
    "    \n",
    "#The default values as specifed by the DDPM paper(https://arxiv.org/pdf/2006.11239) are constants chosen \n",
    "#to be small relative to data scaled to [−1, 1] \n",
    "    \n",
    "    #Assumes fully connected graph\n",
    "    row =torch.arange(0,n).repeat_interleave(n).to(device)\n",
    "    col =torch.arange(0,n).repeat(n).to(device)\n",
    "    edge_id=(row,col)\n",
    "\n",
    "    return frames, seq, edge_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64609c75-1a1a-4ce7-8349-6ea40669a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prot_eGNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, process_device, gnn_device, embed_dim, sc_dim, in_node_nf, hidden_nf, out_node_nf,\n",
    "                 T, b_initial=0.0001, b_final=0.02, \n",
    "                 in_edge_nf=0, act_fn=nn.SiLU(), n_layers=6, residual=True, attention=True, normalize=False, tanh=False                 \n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sc_dim = sc_dim\n",
    "        self.process_device = process_device\n",
    "        self.gnn_device = gnn_device\n",
    "        self.T = T\n",
    "        \n",
    "        self.embedding = nn.Embedding(20,embed_dim)\n",
    "        self.diffusion = Diffusion(T, b_initial=0.0001, b_final=0.02, device=self.process_device) \n",
    "        self.EGNN = eGNN.EGNN(in_node_nf, hidden_nf, out_node_nf, in_edge_nf=0, \n",
    "                              device=self.gnn_device , act_fn=nn.SiLU(), n_layers=n_layers, residual=residual, attention=attention, normalize=normalize, tanh=tanh)\n",
    "\n",
    "    def _sc_embed(self, t):\n",
    "        half_dim = self.sc_dim//2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim,) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "    def sc_pos_embed(self, h):\n",
    "        t = torch.arange(0,h.shape[0])+1\n",
    "        half_dim = h.shape[-1]//2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim,) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        h += emb\n",
    "        return h\n",
    "        \n",
    "    def sample(self, seq_id):\n",
    "        #seq_id is a 1D tensor\n",
    "        x_t = torch.randn((seq_id.shape[0], 4, 3)).to(self.gnn_device) #N x 4 x 3\n",
    "        \n",
    "        #This remains constant, no need to recompute\n",
    "        n_aa = seq_id.shape[0]\n",
    "        seq_id=seq_id.to(self.gnn_device)\n",
    "        h = self.embedding(seq_id)\n",
    "        h = h.to(self.process_device)\n",
    "        h = self.sc_pos_embed(h) \n",
    "\n",
    "        #Generate Edges\n",
    "        row =torch.arange(0,n_aa).repeat_interleave(n_aa).to(self.gnn_device)\n",
    "        col =torch.arange(0,n_aa).repeat(n_aa).to(self.gnn_device)\n",
    "        edges_id=(row,col)\n",
    "        \n",
    "        for t in range(self.T-1, -1, -1):  \n",
    "            if t > 0:\n",
    "                z = torch.normal(mean=torch.zeros_like(x_t),std=torch.ones_like(x_t))\n",
    "            else:\n",
    "                z = torch.zeros_like(x_t)\n",
    "            sc_emb = self._sc_embed(torch.tensor([t+1])).repeat(n_aa, 1)\n",
    "            h_t = torch.cat((h,sc_emb),dim=-1)\n",
    "            h_t = h_t.to(self.gnn_device)\n",
    "            pred_h_eps, pred_x_eps= self.EGNN(h_t, x_t, edges_id, edge_attr=None)\n",
    "            if t != 0:\n",
    "                a_mul = torch.prod(1-self.diffusion.beta[0:t+1])\n",
    "                a_mul_1 = torch.prod(1-self.diffusion.beta[0:t])\n",
    "            else:\n",
    "                a_mul = torch.prod(1-self.diffusion.beta[t])\n",
    "                a_mul_1 = 0\n",
    "            a = 1-self.diffusion.beta[t]\n",
    "            #N.B. Vars = mul(Bt) for Xo ~ N(0,I) if Xo is a predetermined point the defined vars is prefered. See p.g.,3 of the DDPM paper\n",
    "            std = ((1-a_mul_1)/(1-a_mul))*self.diffusion.beta[t]\n",
    "            std= torch.sqrt(std)\n",
    "            x_t = (a**(-0.5))*(x_t- ((1-a)/(torch.sqrt(1-a_mul)))*pred_x_eps)+std*z\n",
    "        return x_t #Multiply the output by the global std\n",
    "        \n",
    "    def forward(self, x, seq_id, edges_id):\n",
    "        assert x.shape[0]==seq_id.shape[0], \"Sequence length and coordinate first dimension is not of the same shape!\"\n",
    "\n",
    "        n_aa = seq_id.shape[0]\n",
    "        seq_id=seq_id.to(self.gnn_device)\n",
    "        h = self.embedding(seq_id)\n",
    "        h = h.to(self.process_device)\n",
    "        h = self.sc_pos_embed(h)\n",
    "        \n",
    "        t2 = torch.randint(0, high=self.T, size=(1,))\n",
    "        sc_emb = self._sc_embed(t2+1).repeat(n_aa, 1)\n",
    "        x_perturbed, x_eps, h_perturbed, h_eps = self.diffusion(x,h,t2)\n",
    "        x_perturbed=x_perturbed.to(self.gnn_device)\n",
    "        h = torch.cat((h,sc_emb),dim=-1)\n",
    "        h = h.to(self.gnn_device)\n",
    "        #h_perturbed=h_perturbed.to(self.gnn_device)\n",
    "        pred_h_eps, pred_x_eps= self.EGNN(h, x_perturbed, edges_id, edge_attr=None)\n",
    "        return pred_h_eps, h_eps, pred_x_eps, x_eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90417a4e-a339-4ad0-8383-fdef90d0bee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Params\n",
    "#Num of clean proteins 31065\n",
    "steps = 300000\n",
    "batch_size = 500\n",
    "with open(list_path) as file:\n",
    "    lines = [line.rstrip() for line in file]\n",
    "np.random.seed(seed=17)\n",
    "np.random.shuffle(lines)\n",
    "train_list = np.array(lines[:-len(lines)//5])\n",
    "val_list = np.array(lines[-len(lines)//5:])\n",
    "np.savetxt(\"train_list.csv\", train_list, delimiter=\",\", fmt='%s')\n",
    "np.savetxt(\"val_list.csv\", val_list, delimiter=\",\", fmt='%s')\n",
    "#Instantiate Models \n",
    "embed_dim = 64\n",
    "sc_dim = 32\n",
    "in_node_nf = embed_dim + sc_dim\n",
    "hidden_nf =128\n",
    "out_node_nf = 64\n",
    "\n",
    "model = prot_eGNN(\"cpu\", \"cuda\" , embed_dim, sc_dim, in_node_nf, hidden_nf, out_node_nf,\n",
    "                 1000, b_initial=0.0001, b_final=0.02, \n",
    "                 in_edge_nf=0, act_fn=nn.SiLU(), n_layers=5, residual=True, attention=True, normalize=False, tanh=False)\n",
    "model=model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c57d036b-5d09-48fc-9f7e-050dcac347e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define loss here\n",
    "denoising_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.01, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2d1d0bd-245f-492a-996b-e1985131f9a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                                             | 500/30000 [00:20<25:58, 18.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Step : 0.0792664960026741\n",
      "500 Step : tensor([0.5158], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                           | 1001/30000 [00:39<23:15, 20.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Step : 0.007118523120880127\n",
      "1000 Step : tensor([0.2666], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▉                                                                          | 1502/30000 [00:58<17:46, 26.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 Step : 0.6716586351394653\n",
      "1500 Step : tensor([0.2524], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▏                                                                        | 2001/30000 [01:17<19:15, 24.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 Step : 0.004300819244235754\n",
      "2000 Step : tensor([0.2379], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▌                                                                       | 2504/30000 [01:37<15:30, 29.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 Step : 0.006018474232405424\n",
      "2500 Step : tensor([0.2585], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▊                                                                      | 3001/30000 [01:56<16:51, 26.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 Step : 0.33960768580436707\n",
      "3000 Step : tensor([1.3580], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████                                                                     | 3501/30000 [02:14<20:20, 21.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 Step : 1.0345332622528076\n",
      "3500 Step : tensor([0.3163], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▍                                                                   | 4000/30000 [02:34<18:53, 22.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 Step : 1.0631515979766846\n",
      "4000 Step : tensor([0.2767], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████▋                                                                  | 4503/30000 [02:52<15:12, 27.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500 Step : 0.9023317098617554\n",
      "4500 Step : tensor([0.2356], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████                                                                 | 5001/30000 [03:11<18:22, 22.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 Step : 0.030750880017876625\n",
      "5000 Step : tensor([0.2582], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████▎                                                               | 5503/30000 [03:31<18:58, 21.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500 Step : 0.6244518160820007\n",
      "5500 Step : tensor([0.2727], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▌                                                              | 6000/30000 [03:50<18:00, 22.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 Step : 0.006573216058313847\n",
      "6000 Step : tensor([0.2709], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████████▉                                                             | 6500/30000 [04:09<16:26, 23.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500 Step : 0.0015672928420826793\n",
      "6500 Step : tensor([0.2431], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████▏                                                           | 7004/30000 [04:28<13:02, 29.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 Step : 0.016375526785850525\n",
      "7000 Step : tensor([0.2604], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████▍                                                          | 7498/30000 [04:46<12:34, 29.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500 Step : 0.007838092744350433\n",
      "7500 Step : tensor([0.2460], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████▊                                                         | 8001/30000 [05:05<15:29, 23.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 Step : 2.4464213848114014\n",
      "8000 Step : tensor([431.8011], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████                                                        | 8500/30000 [05:24<14:41, 24.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500 Step : 0.11232990771532059\n",
      "8500 Step : tensor([0.4028], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▍                                                      | 9001/30000 [05:44<18:06, 19.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 Step : 0.5898489356040955\n",
      "9000 Step : tensor([0.5730], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████▋                                                     | 9500/30000 [06:03<18:34, 18.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9500 Step : 1.641086220741272\n",
      "9500 Step : tensor([0.3901], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████████▉                                                    | 9998/30000 [06:21<10:26, 31.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Step : 0.15516397356987\n",
      "10000 Step : tensor([0.3949], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████▏                                                | 10996/30000 [06:59<10:16, 30.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000 Step : 0.12483146786689758\n",
      "11000 Step : tensor([0.8734], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|█████████████████████████████▌                                               | 11499/30000 [11:33<09:01, 34.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11500 Step : 0.06993775814771652\n",
      "11500 Step : tensor([0.3539], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|█████████████████████████████▌                                               | 11499/30000 [13:45<22:08, 13.93it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m v_i_seq \u001b[38;5;241m=\u001b[39m data_read\u001b[38;5;241m.\u001b[39mencode(v_seq)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 42\u001b[0m     coords\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_i_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     coords\u001b[38;5;241m=\u001b[39mcoords\u001b[38;5;241m*\u001b[39mdata_std\n\u001b[1;32m     44\u001b[0m res \u001b[38;5;241m=\u001b[39m generate_res_object(v_seq,coords)\n",
      "Cell \u001b[0;32mIn[7], line 55\u001b[0m, in \u001b[0;36mprot_eGNN.sample\u001b[0;34m(self, seq_id)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):  \n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m         z \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(x_t)\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/torch/utils/_device.py:79\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training Loop 1\n",
    "g_pdb_dir=\"/mnt/rna01/nico/dynamics_project/learn/eGNN/generated_pdb/\"\n",
    "model.train()\n",
    "agg_loss = torch.tensor([0.],dtype=torch.float32).to(\"cuda\")\n",
    "for i in tqdm(range(steps)):\n",
    "    pdb_path = pdb_dir+np.random.choice(train_list)\n",
    "    try:\n",
    "        frames,seq,edges=input_generation(pdb_path)\n",
    "    except:\n",
    "        continue\n",
    "    CoM = torch.mean(frames.flatten(end_dim=-2),dim=-2, keepdim=True)\n",
    "    frames = frames - CoM[None,:]\n",
    "    frames = frames/data_std\n",
    "    i_seq = data_read.encode(seq)\n",
    "    \n",
    "    pred_h_eps, h_eps, pred_x_eps, x_eps=model(frames, i_seq, edges)\n",
    "\n",
    "    \n",
    "    x_eps = x_eps.to(\"cuda\")\n",
    "    loss = denoising_loss(pred_x_eps,x_eps)\n",
    "    agg_loss +=loss\n",
    "    if (i+1)%500 == 0:\n",
    "        print(f\"{i+1} Step : {loss}\")\n",
    "        print(f\"{i+1} Step : {agg_loss/500}\")\n",
    "        print(type(agg_loss))\n",
    "        agg_loss = torch.tensor([0.],dtype=torch.float32).to(\"cuda\")\n",
    "        torch.save({\n",
    "            'epoch': i+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'step_loss': agg_loss\n",
    "            }, f'./model_weights/prot_eGNN_{i+1}')\n",
    "        model.eval()\n",
    "        if i>9999:\n",
    "            for j in val_list[:30]:\n",
    "                v_frames,v_seq,v_edges=input_generation(pdb_dir+j)\n",
    "                del v_frames\n",
    "                del v_edges\n",
    "                gc.collect()\n",
    "                v_i_seq = data_read.encode(v_seq)\n",
    "                with torch.no_grad():\n",
    "                    coords=model.sample(v_i_seq)\n",
    "                    coords=coords*data_std\n",
    "                res = generate_res_object(v_seq,coords)\n",
    "                generate_pdb(g_pdb_dir+f\"pred_{j}_model_{i}\",res)  \n",
    "        model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba19740-d100-468c-9d76-0c78ab149d76",
   "metadata": {},
   "source": [
    "# Generate Some PDB Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c304689a-d860-4b88-929b-36106bd7ec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                           | 502/300000 [00:19<3:39:01, 22.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Step : 0.004494435619562864\n",
      "500 Step : tensor([0.4007], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                          | 1001/300000 [00:38<3:52:45, 21.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Step : 0.001537179690785706\n",
      "1000 Step : tensor([0.3827], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                          | 1502/300000 [00:57<2:56:47, 28.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 Step : 0.0006717611104249954\n",
      "1500 Step : tensor([0.3940], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                          | 2001/300000 [01:15<3:42:01, 22.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 Step : 0.033106159418821335\n",
      "2000 Step : tensor([0.3630], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                          | 2503/300000 [01:34<4:00:26, 20.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 Step : 0.00017445038247387856\n",
      "2500 Step : tensor([0.3373], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                          | 3001/300000 [01:58<3:58:15, 20.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 Step : 0.7206881642341614\n",
      "3000 Step : tensor([0.3099], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                          | 3501/300000 [02:22<5:21:36, 15.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 Step : 0.05715889111161232\n",
      "3500 Step : tensor([0.2992], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                          | 4002/300000 [02:43<3:51:04, 21.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 Step : 0.044561609625816345\n",
      "4000 Step : tensor([0.3126], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▏                                                                         | 4502/300000 [03:01<3:00:07, 27.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500 Step : 0.0032121739350259304\n",
      "4500 Step : tensor([110.3153], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                                         | 5001/300000 [03:19<3:31:36, 23.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 Step : 0.40564629435539246\n",
      "5000 Step : tensor([199.8823], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                         | 5502/300000 [03:38<3:09:32, 25.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500 Step : 1.1418873071670532\n",
      "5500 Step : tensor([0.3448], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                         | 6004/300000 [03:57<3:03:03, 26.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 Step : 0.0021270005963742733\n",
      "6000 Step : tensor([0.5229], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                         | 6500/300000 [04:15<3:17:36, 24.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500 Step : 0.00012239243369549513\n",
      "6500 Step : tensor([0.3326], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                         | 7004/300000 [04:34<2:50:52, 28.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 Step : 0.01342146098613739\n",
      "7000 Step : tensor([0.4649], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▉                                                                         | 7500/300000 [04:51<3:07:21, 26.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500 Step : 0.00013185612624511123\n",
      "7500 Step : tensor([0.4135], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                         | 8004/300000 [05:10<2:53:05, 28.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 Step : 0.01567033864557743\n",
      "8000 Step : tensor([0.7253], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                         | 8039/300000 [05:11<3:08:36, 25.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m x_eps \u001b[38;5;241m=\u001b[39m x_eps\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m denoising_loss(pred_x_eps,x_eps)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m agg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m%\u001b[39mbatch_size \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/torch/_tensor.py:512\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m        used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/torch/overrides.py:1630\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1627\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1629\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1630\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/torch/utils/_device.py:79\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gvp/lib/python3.11/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop 2\n",
    "g_pdb_dir=\"/mnt/rna01/nico/dynamics_project/learn/eGNN/generated_pdb/\"\n",
    "model.train()\n",
    "agg_loss = torch.tensor([0.],dtype=torch.float32).to(\"cuda\")\n",
    "for i in tqdm(range(steps)):\n",
    "    pdb_path = pdb_dir+np.random.choice(train_list)\n",
    "    try:\n",
    "        frames,seq,edges=input_generation(pdb_path)\n",
    "    except:\n",
    "        continue\n",
    "    if len(seq)>300:continue\n",
    "    CoM = torch.mean(frames.flatten(end_dim=-2),dim=-2, keepdim=True)\n",
    "    frames = frames - CoM[None,:]\n",
    "    frames = frames/data_std\n",
    "    i_seq = data_read.encode(seq)\n",
    "    \n",
    "    pred_h_eps, h_eps, pred_x_eps, x_eps=model(frames, i_seq, edges)\n",
    "\n",
    "    \n",
    "    x_eps = x_eps.to(\"cuda\")\n",
    "    loss = denoising_loss(pred_x_eps,x_eps)\n",
    "    loss.backward()\n",
    "    agg_loss +=loss\n",
    "    if (i+1)%batch_size ==0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    if (i+1)%500 == 0:\n",
    "        print(f\"{i+1} Step : {loss}\")\n",
    "        print(f\"{i+1} Step : {agg_loss/500}\")\n",
    "        print(type(agg_loss))\n",
    "        torch.save({\n",
    "            'epoch': i+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'step_loss': agg_loss[0]\n",
    "            }, f'./model_weights/prot_eGNN_{i+1}')\n",
    "        agg_loss = torch.tensor([0.],dtype=torch.float32).to(\"cuda\")\n",
    "        if i>9999:\n",
    "            model.eval()\n",
    "            for j in val_list[:30]:\n",
    "                v_frames,v_seq,v_edges=input_generation(pdb_dir+j)\n",
    "                del v_frames\n",
    "                del v_edges\n",
    "                gc.collect()\n",
    "                v_i_seq = data_read.encode(v_seq)\n",
    "                with torch.no_grad():\n",
    "                    coords=model.sample(v_i_seq)\n",
    "                    coords=coords*data_std\n",
    "                res = generate_res_object(v_seq,coords)\n",
    "                generate_pdb(g_pdb_dir+f\"pred_{j+1}_model_{i}\",res)  \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f0f69-09e1-4f90-b402-4be5ba5763d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for i in val_list[:10]:\n",
    "    v_frames,v_seq,v_edges=input_generation(pdb_dir+i)\n",
    "    del v_frames\n",
    "    del v_edges\n",
    "    gc.collect()\n",
    "    v_i_seq = data_read.encode(v_seq)\n",
    "    with torch.no_grad():\n",
    "        coords=model.sample(v_i_seq)\n",
    "        coords=coords*10\n",
    "    res = generate_res_object(v_seq,coords)\n",
    "    generate_pdb(g_pdb_dir+i,res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
